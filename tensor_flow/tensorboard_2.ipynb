{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djUvWu41mtXa"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "su2RaORHpReL"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NztQK2uFpXT-"
      },
      "source": [
        "# TensorBoard Scalars: Logging training metrics in Keras\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tensorboard/scalars_and_keras\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/scalars_and_keras.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorboard/blob/master/docs/scalars_and_keras.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDXRFe_qp5C3"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Machine learning invariably involves understanding key metrics such as loss and how they change as training progresses. These metrics can help you understand if you're [overfitting](https://en.wikipedia.org/wiki/Overfitting), for example, or if you're unnecessarily training for too long. You may want to compare these metrics across different training runs to help debug and improve your model.\n",
        "\n",
        "TensorBoard's **Time Series Dashboard** allows you to visualize these metrics using a simple API with very little effort. This tutorial presents very basic examples to help you learn how to use these APIs with TensorBoard when developing your Keras model. You will learn how to use the Keras TensorBoard callback and TensorFlow Summary APIs to visualize default and custom scalars."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG-nnZK9qW9z"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3U5gdCw_nSG3"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1qIKtOBrqc9Y"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-23 21:58:00.758625: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-23 21:58:00.765624: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1748051880.773747  203251 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1748051880.776078  203251 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1748051880.782517  203251 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1748051880.782527  203251 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1748051880.782528  203251 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1748051880.782529  203251 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-05-23 21:58:00.784999: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version:  2.19.0\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from packaging import version\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import backend as K\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print(\"TensorFlow version: \", tf.__version__)\n",
        "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
        "    \"This notebook requires TensorFlow 2.0 or above.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UbFM4dlnGB3S"
      },
      "outputs": [],
      "source": [
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YDAoNCN3ZNS"
      },
      "source": [
        "## Set up data for a simple regression\n",
        "\n",
        "You're now going to use [Keras](https://www.tensorflow.org/guide/keras) to calculate a regression, i.e., find the best line of fit for a paired data set. (While using neural networks and gradient descent is [overkill for this kind of problem](https://stats.stackexchange.com/questions/160179/do-we-need-gradient-descent-to-find-the-coefficients-of-a-linear-regression-mode), it does make for a very easy to understand example.)\n",
        "\n",
        "You're going to use TensorBoard to observe how training and test **loss** change across epochs. Hopefully, you'll see training and test loss decrease over time and then remain steady.\n",
        "\n",
        "First, generate 1000 data points roughly along the line *y = 0.5x + 2*. Split these data points into training and test sets. Your hope is that the neural net learns this relationship."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j-ryO6OxnQH_"
      },
      "outputs": [],
      "source": [
        "data_size = 1000\n",
        "# 80% of the data is for training.\n",
        "train_pct = 0.8\n",
        "\n",
        "train_size = int(data_size * train_pct)\n",
        "\n",
        "# Create some input data between -1 and 1 and randomize it.\n",
        "x = np.linspace(-1, 1, data_size)\n",
        "np.random.shuffle(x)\n",
        "\n",
        "# Generate the output data.\n",
        "# y = 0.5x + 2 + noise\n",
        "y = 0.5 * x + 2 + np.random.normal(0, 0.05, (data_size, ))\n",
        "\n",
        "# Split into test and train pairs.\n",
        "x_train, y_train = x[:train_size], y[:train_size]\n",
        "x_test, y_test = x[train_size:], y[train_size:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je59_8Ts3rq0"
      },
      "source": [
        "## Training the model and logging loss\n",
        "\n",
        "You're now ready to define, train and evaluate your model. \n",
        "\n",
        "To log the *loss* scalar as you train, you'll do the following:\n",
        "\n",
        "1.   Create the Keras [TensorBoard callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard)\n",
        "2.   Specify a log directory\n",
        "3.   Pass the TensorBoard callback to Keras' [Model.fit()](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#fit).\n",
        "\n",
        "TensorBoard reads log data from the log directory hierarchy. In this notebook, the root log directory is ```logs/scalars```, suffixed by a timestamped subdirectory. The timestamped subdirectory enables you to easily identify and select training runs as you use TensorBoard and iterate on your model.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VmEQwCon3i7m"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "I0000 00:00:1748051882.150370  203251 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 954 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training ... With default parameters, this takes less than 10 seconds.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1748051882.701379  203393 service.cc:152] XLA service 0x70c3c0004760 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1748051882.701394  203393 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
            "2025-05-23 21:58:02.710838: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "I0000 00:00:1748051882.718907  203393 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
            "I0000 00:00:1748051882.839861  203393 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average test loss:  0.05056760617066175\n"
          ]
        }
      ],
      "source": [
        "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(16, input_dim=1),\n",
        "    keras.layers.Dense(1),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='mse', # keras.losses.mean_squared_error\n",
        "    optimizer=keras.optimizers.SGD(learning_rate=0.2),\n",
        ")\n",
        "\n",
        "print(\"Training ... With default parameters, this takes less than 10 seconds.\")\n",
        "training_history = model.fit(\n",
        "    x_train, # input\n",
        "    y_train, # output\n",
        "    batch_size=train_size,\n",
        "    verbose=0, # Suppress chatty output; use Tensorboard instead\n",
        "    epochs=100,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[tensorboard_callback],\n",
        ")\n",
        "\n",
        "print(\"Average test loss: \", np.average(training_history.history['loss']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "042k7GMERVkx"
      },
      "source": [
        "## Examining loss using TensorBoard\n",
        "\n",
        "Now, start TensorBoard, specifying the root log directory you used above.\n",
        "\n",
        "Wait a few seconds for TensorBoard's UI to spin up. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6pck56gKReON"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-fbe0e6313cb150df\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-fbe0e6313cb150df\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6007;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%tensorboard --logdir logs/scalars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmQHlG10Kpu2"
      },
      "source": [
        "<!-- <img class=\"tfo-display-only-on-site\" src=\"https://github.com/tensorflow/tensorboard/blob/master/docs/images/scalars_loss.png?raw=1\"/> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciSIRibhRi6N"
      },
      "source": [
        "You may see TensorBoard display the message \"No dashboards are active for the current data set\". That's because initial logging data hasn't been saved yet. As training progresses, the Keras model will start logging data. TensorBoard will periodically refresh and show you your scalar metrics. If you're impatient, you can tap the Refresh arrow at the top right.\n",
        "\n",
        "As you watch the training progress, note how both training and validation loss rapidly decrease, and then remain stable. In fact, you could have stopped training after 25 epochs, because the training didn't improve much after that point.\n",
        "\n",
        "Hover over the graph to see specific data points. You can also try zooming in with your mouse, or selecting part of them to view more detail.\n",
        "\n",
        "Notice the \"Runs\" selector on the left. A \"run\" represents a set of logs from a round of training, in this case the result of Model.fit(). Developers typically have many, many runs, as they experiment and develop their model over time. \n",
        "\n",
        "Use the Runs selector to choose specific runs, or choose from only training or validation. Comparing runs will help you evaluate which version of your code is solving your problem better.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "finK0GfYyefe"
      },
      "source": [
        "Ok, TensorBoard's loss graph demonstrates that the loss consistently decreased for both training and validation and then stabilized. That means that the model's metrics are likely very good! Now see how the model actually behaves in real life. \n",
        "\n",
        "Given the input data (60, 25, 2), the line *y = 0.5x + 2* should yield (32, 14.5, 3). Does the model agree?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EuiLgxQstt32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
            "[[32.381664 ]\n",
            " [14.659373 ]\n",
            " [ 3.0132937]]\n"
          ]
        }
      ],
      "source": [
        "print(model.predict(tf.convert_to_tensor([60, 25, 2])))\n",
        "# True values to compare predictions against: \n",
        "# [[32.0]\n",
        "#  [14.5]\n",
        "#  [ 3.0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bom4MdeewRKS"
      },
      "source": [
        "Not bad!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvwGmJK9XWmh"
      },
      "source": [
        "## Logging custom scalars\n",
        "\n",
        "What if you want to log custom values, such as a [dynamic learning rate](https://www.jeremyjordan.me/nn-learning-rate/)? To do that, you need to use the TensorFlow Summary API.\n",
        "\n",
        "Retrain the regression model and log a custom learning rate. Here's how:\n",
        "\n",
        "1.  Create a file writer, using ```tf.summary.create_file_writer()```.\n",
        "2.  Define a custom learning rate function. This will be passed to the Keras [LearningRateScheduler](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler) callback.\n",
        "3.  Inside the learning rate function, use ```tf.summary.scalar()``` to log the custom learning rate.\n",
        "4.  Pass the LearningRateScheduler callback to Model.fit().\n",
        "\n",
        "In general, to log a custom scalar, you need to use ```tf.summary.scalar()``` with a file writer. The file writer is responsible for writing data for this run to the specified directory and is implicitly used when you use the ```tf.summary.scalar()```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XB95ltRiXVXk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "file_writer = tf.summary.create_file_writer(logdir + \"/metrics\")\n",
        "file_writer.set_as_default()\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "  \"\"\"\n",
        "  Returns a custom learning rate that decreases as epochs progress.\n",
        "  \"\"\"\n",
        "  learning_rate = 0.2\n",
        "  if epoch > 10:\n",
        "    learning_rate = 0.02\n",
        "  if epoch > 20:\n",
        "    learning_rate = 0.01\n",
        "  if epoch > 50:\n",
        "    learning_rate = 0.005\n",
        "\n",
        "  tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
        "  return learning_rate\n",
        "\n",
        "lr_callback = keras.callbacks.LearningRateScheduler(lr_schedule)\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(16, input_dim=1),\n",
        "    keras.layers.Dense(1),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='mse', # keras.losses.mean_squared_error\n",
        "    optimizer=keras.optimizers.SGD(),\n",
        ")\n",
        "\n",
        "training_history = model.fit(\n",
        "    x_train, # input\n",
        "    y_train, # output\n",
        "    batch_size=train_size,\n",
        "    verbose=0, # Suppress chatty output; use Tensorboard instead\n",
        "    epochs=100,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[tensorboard_callback, lr_callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pck8OQEjayDM"
      },
      "source": [
        "Let's look at TensorBoard again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0sjM2wXGa0mF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6007 (pid 217184), started 0:00:06 ago. (Use '!kill 217184' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-ce2ea2b7ccca3538\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-ce2ea2b7ccca3538\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6007;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%tensorboard --logdir logs/scalars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkIahGZKK9I7"
      },
      "source": [
        "<!-- <img class=\"tfo-display-only-on-site\" src=\"https://github.com/tensorflow/tensorboard/blob/master/docs/images/scalars_custom_lr.png?raw=1\"/> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRlUDnhlkN_q"
      },
      "source": [
        "Using the \"Runs\" selector on the left, notice that you have a ```<timestamp>/metrics``` run. Selecting this run displays a \"learning rate\" graph that allows you to verify the progression of the learning rate during this run. \n",
        "\n",
        "You can also compare this run's training and validation loss curves against your earlier runs.\n",
        "You might also notice that the learning rate schedule returned discrete values, depending on epoch, but the learning rate plot may appear smooth.  TensorBoard has a smoothing parameter that you may need to turn down to zero to see the unsmoothed values. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0TTI16Nl0nk"
      },
      "source": [
        "How does this model do?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "97T4vT3QkQJH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
            "[[32.341713 ]\n",
            " [14.642725 ]\n",
            " [ 3.0119662]]\n"
          ]
        }
      ],
      "source": [
        "print(model.predict(tf.convert_to_tensor([60, 25, 2])))\n",
        "# True values to compare predictions against: \n",
        "# [[32.0]\n",
        "#  [14.5]\n",
        "#  [ 3.0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxAVfW8lhZ0e"
      },
      "source": [
        "## Batch-level logging\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-ltiSuypLVE"
      },
      "source": [
        "First let's load the MNIST dataset, normalize the data and write a function that creates a simple Keras model for classifying the images into 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "N2kbowJTpJWJ"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "def create_model():\n",
        "  return tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68pV5ZRe1iZ7"
      },
      "source": [
        "### Instantaneous batch-level logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_QYrBgkTsLH"
      },
      "source": [
        "Logging metrics at the batch level instantaneously can show us the level of fluctuation between batches while training in each epoch, which can be useful for debugging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hXsbsXDqgvA"
      },
      "source": [
        "Setting up a summary writer to a different log directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7OTD7Vpg2DLv"
      },
      "outputs": [],
      "source": [
        "log_dir = 'logs/batch_level/' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '/train'\n",
        "train_writer = tf.summary.create_file_writer(log_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlVZY2VqrK9D"
      },
      "source": [
        "To enable batch-level logging, custom `tf.summary` metrics should be defined by overriding `train_step()` in the Model's class definition and enclosed in a summary writer context. This can simply be made combined into subclassed Model definitions or can extend to edit our previous Functional API Model, as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IGcNr1ZS1xXL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, model):\n",
        "    super().__init__()\n",
        "    self.step_counter = tf.Variable(0, trainable=False, dtype=tf.int64)\n",
        "    self.model = model\n",
        "  \n",
        "  def train_step(self, data):\n",
        "    x, y = data\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_pred = self.model(x, training=True)\n",
        "      loss = self.compiled_loss(y, y_pred)\n",
        "      #mse = tf.keras.losses.mean_squared_error(y, K.max(y_pred, axis=-1))\n",
        "      mse = tf.keras.losses.mse(y, tf.reduce_max(y_pred, axis=-1))\n",
        "    #self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n",
        "    # Compute gradients\n",
        "    gradients = tape.gradient(loss, self.trainable_variables)\n",
        "    # Update weights\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "    \n",
        "    # Increment step counter\n",
        "    self.step_counter.assign_add(1)\n",
        "    \n",
        "    with train_writer.as_default(step=self.step_counter):\n",
        "      tf.summary.scalar('batch_loss', loss)\n",
        "      tf.summary.scalar('batch_mse', mse)\n",
        "    return self.compute_metrics(x, y, y_pred, None)\n",
        "  \n",
        "  def call(self, x):\n",
        "    x = self.model(x)\n",
        "    return x\n",
        "\n",
        "# Adds custom batch-level metrics to our previous Functional API model\n",
        "model = MyModel(create_model())\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wqqNaXP2bLc"
      },
      "source": [
        "Define our TensorBoard callback to log both epoch-level and batch-level metrics to our log directory and call `model.fit()` with our selected `batch_size`: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YXK-iE0e2UOE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:671: UserWarning: `model.compiled_loss()` is deprecated. Instead, use `model.compute_loss(x, y, y_pred, sample_weight, training)`.\n",
            "  warnings.warn(\n",
            "2025-05-23 21:58:16.136040: W tensorflow/core/framework/op_kernel.cc:1857] OP_REQUIRES failed at xla_ops.cc:529 : INVALID_ARGUMENT: Trying to access resource _AnonymousVar825 (defined @ /tmp/ipykernel_203251/2384580617.py:2) located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0\n",
            " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n",
            "2025-05-23 21:58:16.136060: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Trying to access resource _AnonymousVar825 (defined @ /tmp/ipykernel_203251/2384580617.py:2) located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0\n",
            " Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n",
            "\t [[{{node StatefulPartitionedCall}}]]\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n\n  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3098, in run_cell\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3153, in _run_cell\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3365, in run_cell_async\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3610, in run_ast_nodes\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3670, in run_code\n\n  File \"/tmp/ipykernel_203251/809734099.py\", line 3, in <module>\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\nTrying to access resource _AnonymousVar825 (defined @ /tmp/ipykernel_203251/2384580617.py:2) located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0\n Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_17286]",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m          \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[32m     54\u001b[39m                                       inputs, attrs, num_outputs)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[31mInvalidArgumentError\u001b[39m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n\n  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3098, in run_cell\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3153, in _run_cell\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3365, in run_cell_async\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3610, in run_ast_nodes\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3670, in run_code\n\n  File \"/tmp/ipykernel_203251/809734099.py\", line 3, in <module>\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/home/oleg/Developer/GitHub/TensorFlow/TensorFlow/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\nTrying to access resource _AnonymousVar825 (defined @ /tmp/ipykernel_203251/2384580617.py:2) located in device /job:localhost/replica:0/task:0/device:CPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0\n Cf. https://www.tensorflow.org/xla/known_issues#tfvariable_on_a_different_device\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_17286]"
          ]
        }
      ],
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "\n",
        "model.fit(x=x_train, \n",
        "          y=y_train,\n",
        "          epochs=5,\n",
        "          batch_size=500, \n",
        "          validation_data=(x_test, y_test), \n",
        "          callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsxwF83h2kiM"
      },
      "source": [
        "Open TensorBoard with the new log directory and see both the epoch-level and batch-level metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlcafPNY2oUW"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/batch_level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReO1LD-g2vgk"
      },
      "source": [
        "### Cumulative batch-level logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0eE_yjt45TN"
      },
      "source": [
        "Batch-level logging can also be implemented cumulatively, averaging each batch's metrics with those of previous batches and resulting in a smoother training curve when logging batch-level metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He88QGuu25Vm"
      },
      "source": [
        "Setting up a summary writer to a different log directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX3nsdqi28W1"
      },
      "outputs": [],
      "source": [
        "log_dir = 'logs/batch_avg/' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '/train'\n",
        "train_writer = tf.summary.create_file_writer(log_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHcpdSR6q5MY"
      },
      "source": [
        "Create stateful metrics that can be logged per batch:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cAiVu_KjOVi"
      },
      "outputs": [],
      "source": [
        "batch_loss = tf.keras.metrics.Mean('batch_loss', dtype=tf.float32)\n",
        "batch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('batch_accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8ESFevo3Vjz"
      },
      "source": [
        "As before, add custom `tf.summary` metrics in the overridden `train_step` method. To make the batch-level logging cumulative, use the stateful metrics we defined to calculate the cumulative result given each training step's data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQ_-46fpjUVl"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, model):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "  \n",
        "  def train_step(self, data):\n",
        "    x, y = data\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_pred = self.model(x, training=True)\n",
        "      loss = self.compiled_loss(y, y_pred)\n",
        "    self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n",
        "    batch_loss(loss)\n",
        "    batch_accuracy(y, y_pred)\n",
        "    with train_writer.as_default(step=self._train_counter):\n",
        "      tf.summary.scalar('batch_loss', batch_loss.result())\n",
        "      tf.summary.scalar('batch_accuracy', batch_accuracy.result())\n",
        "    return self.compute_metrics(x, y, y_pred, None)\n",
        "  \n",
        "  def call(self, x):\n",
        "    x = self.model(x)\n",
        "    return x\n",
        "\n",
        "# Adds custom batch-level metrics to our previous Functional API model\n",
        "model = MyModel(create_model())\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgtSX5E1uNhC"
      },
      "source": [
        "As before, define our TensorBoard callback and call `model.fit()` with our selected `batch_size`: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGWYCUFhjztg"
      },
      "outputs": [],
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "\n",
        "model.fit(x=x_train, \n",
        "          y=y_train,\n",
        "          epochs=5,\n",
        "          batch_size=500, \n",
        "          validation_data=(x_test, y_test), \n",
        "          callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSoDvPoDvAci"
      },
      "source": [
        "Open TensorBoard with the new log directory and see both the epoch-level and batch-level metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYmYfTeSk7AD"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/batch_avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXCePQi7_f50"
      },
      "source": [
        "That's it! You now know how to create custom training metrics in TensorBoard for a wide variety of use cases."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "scalars_and_keras.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
